---
title: "A/B test in R"
output: github_document
editor_options: 
  markdown: 
    wrap: sentence
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Hotel Booking A/B testing

Let's consider a scenario where we have A/B test results from two distinct hotel booking websites.
Our first course of action is to perform a thorough analysis of the data.
Once we have completed the initial analysis, we can then extract meaningful insights from the data and use them to draw conclusions in the second step.
Finally, based on our conclusions, we can offer actionable recommendations or suggestions to the relevant product or management teams.

## Data Set Summary

Variant A represents the control group that showcases the current features or products available on a website.
In contrast, Variant B is from the experimental group that tests a new version of a feature or product to assess its impact on user engagement or conversions (such as bookings).
To evaluate the effectiveness of each variant, the data set includes two defined categories based on logical values: "converted" is registered as "true" when a customer successfully completes a booking, while "false" indicates a customer who visits the website but does not complete a booking.

## Test Hypothesis

The null hypothesis states that both versions A and B have an equal likelihood of driving customer bookings or conversions.
In other words, there is no significant difference or effect between the two versions.
Conversely, the alternative hypothesis suggests that versions A and B have different probabilities of driving customer bookings, indicating a difference between the two versions.
Version B is better than version A in driving customer bookings.
PExp_B!
= Pcont_A.

# Analysis

#### 1.Preparing the dataset and loading libraries

```{r loading}
install.packages("tidyverse") 
library(tidyverse)

ABTest <- read.csv("Website Results.csv", 
                   header = TRUE)
```

#### 2.Let's filter conversions for variants A & B and compute their corresponding conversion rates

```{r filtering}
# conversions for variant_A
conversion_subset_A <- ABTest %>%
	filter(variant == "A" & converted == "TRUE")

# Total Number of Conversions for variant_A
conversions_A <- nrow(conversion_subset_A)

# Number of Visitors for variant_A
visitors_A <- nrow(ABTest %>%
	filter(variant == "A"))

# Conversion_rate_A
conv_rate_A <- (conversions_A/visitors_A)
print(conv_rate_A)

# Subset of conversions for variant_B
conversion_subset_B <- ABTest %>%
	filter(variant == "B" & converted == "TRUE")

# Number of Conversions for variant_B
conversions_B <- nrow(conversion_subset_B)

# Number of Visitors for variant_B
visitors_B <- nrow(ABTest %>%
	filter(variant == "B"))

# Conversion_rate_B
conv_rate_B <- (conversions_B/visitors_B)
print(conv_rate_B) 


```

#### 3.Let's compute the relative uplift using conversion rates A & B. The uplift is a percentage of the increase

```{r increase}
uplift <- (conv_rate_B - conv_rate_A) / conv_rate_A * 100
uplift # 82.72%
```

B is better than A by 83%.
This is high enough to decide a winner.

#### 4.Let's compute the pooled probability, standard error, the margin of error, and difference in proportion (point estimate) for variants A & B

```{r probability}

# Pooled sample proportion for variants A & B
p_pool <- (conversions_A + conversions_B) / (visitors_A +
											visitors_B)
print(p_pool) 

# Let's compute Standard error for variants A & B (SE_pool)
SE_pool <- sqrt(p_pool * (1 - p_pool) * ((1 / visitors_A) +
										(1 / visitors_B)))
print(SE_pool) 

# Let's compute the margin of error for the pool
MOE <- SE_pool * qnorm(0.975)
print(MOE) 

# Point Estimate or Difference in proportion
d_hat <- conv_rate_B - conv_rate_A


```

#### 5.Let's compute the z-score

```{r z_score}

# Compute the Z-score so we
# can determine the p-value
z_score <- d_hat / SE_pool
print(z_score) 


```

#### 6.Using this z-score, we can quickly determine the p-value via a look-up table, or using the code below:

```{r p_value}
# Let's compute p_value
# using the z_score value
p_value <- pnorm(q = -z_score,
				mean = 0,
				sd = 1) * 2
print(p_value) 
```

#### 7.Let's compute the confidence interval for the pool

```{r}
ci <- c(d_hat - MOE, d_hat + MOE)
ci
ci_lower <- d_hat - MOE
ci_lower 
ci_upper <- d_hat + MOE
ci_upper 
```

#### 8.Let's compute Standard Error and Confidence Intervals for Test version A and B separately

```{r interval}
# Let's compute Confidence interval for the
# pool using pre-calculated results
ci <- c(d_hat - MOE, d_hat + MOE)
ci 

# Using same steps as already shown,
# let's compute the confidence
# interval for variants A separately
X_hat_A <- conversions_A / visitors_A
se_hat_A <- sqrt(X_hat_A * (1 - X_hat_A) / visitors_A)
ci_A <- c(X_hat_A - qnorm(0.975) * se_hat_A, X_hat_A
		+ qnorm(0.975) * se_hat_A)
print(ci_A) 

# Using same steps as already shown,
# let's compute the confidence
# interval for variants B separately								
X_hat_B <- conversions_B / visitors_B
se_hat_B <- sqrt(X_hat_B * (1 - X_hat_B) / visitors_B)
ci_B <- c(X_hat_B - qnorm(0.975) * se_hat_B,
		X_hat_B + qnorm(0.975) * se_hat_B)
print(ci_B) 

```

#### 9.Let's extract the lower and upper confidence intervals into `lower` and `upper` respectively

```{r}
ci_lower_A <- X_hat_A - qnorm(0.975)*se_hat_A
ci_lower_A
ci_upper_A <- X_hat_A + qnorm(0.975)*se_hat_A
ci_upper_A
ci_lower_B <- X_hat_B - qnorm(0.975)*se_hat_B
ci_lower_B
```

#### 10.Let's create a dataframe of results for the pool

```{r}
vis_result_pool <- data.frame(
  metric=c(
    'Estimated Difference',
    'Relative Uplift(%)',
    'pooled sample proportion',
    'Standard Error of Difference',
    'z_score',
    'p-value',
    'Margin of Error',
    'CI-lower',
    'CI-upper'),
  value=c(
    conv_rate_B - conv_rate_A,
    uplift,
    p_pool,
    SE_pool,
    z_score,
    p_value,
    MOE,
    ci_lower,
    ci_upper
  ))
vis_result_pool
```

#### 11.Let's visualize results per test version in a table:

```{r viz}
viz_result <- data.frame(variant = c("A","B"), visitors = c(visitors_A, visitors_B),
                         conversions = c(conversions_A,conversions_B),conversion_rate = round(c(conv_rate_A, conv_rate_B),4),
                         Standard_error = round(c(se_hat_A,se_hat_B),5), Conf_Interval_A = c(ci_A[1],ci_A[2]))
viz_result

```

#### 12.Let' visualize the pool using a normal distribution

```{r}
ABTdata_pool = data.frame(Conf_interval = rnorm(n = 10000, mean = 0, sd = SE_pool))
#Let's plot the distributions
b <- ggplot(ABTdata_pool, aes(x = Conf_interval))
b + geom_histogram(aes(y = ..density..),
                   colour="black", fill="white", binwidth = 0.00029) +
  geom_density(alpha = 0.2, fill = "blue")
```

## Summary

Based on the provided data, Variant A had 20 conversions out of 721 hits, while Variant B had 37 conversions out of 730 hits.
This translates to a relative uplift of 82.72% compared to Variant A's conversion rate of 2.77% and Variant B's conversion rate of 5.07%.
The statistical analysis produced a p-value of 0.02448, indicating strong statistical significance in the test results.
As a result, we should reject the null hypothesis and accept Variant B, which performed better than Variant A by 82.72%.
Therefore, it is recommended to roll out Variant B to all users.
